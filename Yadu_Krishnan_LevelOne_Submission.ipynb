{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geekdyout/yadu_gdsc_aiml_submission/blob/main/Yadu_Krishnan_LevelOne_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GDSC - Artificial Intelligence & Machine Learning\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **I. Introduction to AI / ML**\n",
        "Machine learning aims to teach a machine how to perform a specific task and provide accurate results by identifying patterns. Curious how ML does this? There's a lot of math and logic that goes behind what's happening in a Machine Learning model.\n",
        "\n",
        "### **II. Supervised & Unsupervised Machine Learning**\n",
        "Supervised learning is the types of machine learning in which machines are trained using well \"labelled\" training data, and on basis of that data, machines predict the output. The labelled data means some input data is already tagged with the correct output.\n",
        "\n",
        "Supervised learning is a process of providing input data as well as correct output data to the machine learning model. The aim of a supervised learning algorithm is to find a mapping function to map the input variable(x) with the output variable(y).\n",
        "\n",
        "Unsupervised learning, also known as unsupervised machine learning, uses machine learning algorithms to analyze and cluster unlabeled datasets. These algorithms discover hidden patterns or data groupings without the need for human intervention.\n",
        "\n",
        "### **III. Level One**\n",
        "For level one, we would be expecting you to research on your own and complete this assignment. We will not be directly explaining the concepts and you would be expected to try to learn it on your own, however, if you do have any doubts regarding this, you can contact Dhruv Shah or Advik Raj Basani. We would be dealing with ONLY Supervised Machine Learning this assignment, and the math involved in this has already been covered either in first year / 12th grade. Math concepts we would expect you to know before you go into this assignment is:\n",
        "\n",
        "*   Multiplication of Matrices\n",
        "*   Dot Product & Cross Product\n",
        "\n",
        "We would also need to know the very basics of Python. This is not a very difficult task and you can familiarise yourself with how Python works with this link: https://www.w3schools.com/python/\n",
        "\n",
        "### **IV. How to Start**\n",
        "First off, go to File -> Save a copy on Drive, and share the new copied file on YOUR drive (with Editing permissions) and fill out this form with the copied share link.\n",
        "\n",
        "https://forms.gle/D6qigSNLEaPf3kTB6\n",
        "\n",
        "And... you're done! You can now get started and learn how ML works.\n",
        "\n"
      ],
      "metadata": {
        "id": "5TnWQuVzylfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: Linear Regression using Gradient Descent\n",
        "\n",
        "Hey! Welcome to your first assignment question. Let's introduce you to the question.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "You are given a dataset about House Prices in an area. In this dataset, you will have multiple features about the house. You are expected to create a Linear Regression ML model (**ONLY USING NUMPY** - a python library) to predict prices. Below you will find two links - one containing the training dataset and the other containing the test dataset. Your assignment is to fill in the blanks of code and maximize accuracy / minimize loss by using as many features as you can use.\n",
        "\n",
        "Training_Data, Testing_Data & Information_about_Features can be found in this drive link: https://drive.google.com/drive/folders/1jTnYiFaUn0czGEmOS637SWgwaNdSDp07?usp=sharing\n",
        "\n",
        "Resources to study:\n",
        "* https://www.geeksforgeeks.org/ml-linear-regression/\n",
        "* https://www.javatpoint.com/cost-function-in-machine-learning\n",
        "* https://www.javatpoint.com/gradient-descent-in-machine-learning\n",
        "* https://www.scaler.com/topics/np-vectorize/\n",
        "* https://www.simplilearn.com/what-is-multiple-linear-regression-in-machine-learning-article\n",
        "* https://www.javatpoint.com/feature-engineering-for-machine-learning\n"
      ],
      "metadata": {
        "id": "nGsmABrIfdeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "-IWt8Usxfk7S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ConvertToInputOutput(dataframe):\n",
        "  Y = dataframe[['SalePrice']]\n",
        "  chosen_feats = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
        "  X = dataframe[chosen_feats]\n",
        "  return X,Y\n",
        "\n",
        "\n",
        "# Normalization Function\n",
        "def Normalize(X):\n",
        "  return (X - X.mean())/(X.std()+0.001)\n",
        "\n",
        "# Randomize a dataset\n",
        "def randomize_dataset(dataset):\n",
        "    dataset = dataset.sample(frac=1)\n",
        "    return dataset\n",
        "\n",
        "# Split Training and Test Data\n",
        "def obtain_training_test_data(X, Y, n):\n",
        "    num = round(X.shape[0] * (1 - n))\n",
        "\n",
        "    X_train = X.iloc[0:num, :]\n",
        "    X_test = X.iloc[(num):, :]\n",
        "\n",
        "    Y_train = Y.iloc[0:num]\n",
        "    Y_test = Y.iloc[(num):]\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test"
      ],
      "metadata": {
        "id": "IchLUyDhyex3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset\n",
        "train = pd.read_csv(\"/train.csv\")\n",
        "print(train.isnull().sum()[train.isnull().sum() > 0])"
      ],
      "metadata": {
        "id": "OaV3NetxgcNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd7edd7-9e61-4406-c974-7a966ffae870"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LotFrontage      259\n",
            "Alley           1369\n",
            "MasVnrType         8\n",
            "MasVnrArea         8\n",
            "BsmtQual          37\n",
            "BsmtCond          37\n",
            "BsmtExposure      38\n",
            "BsmtFinType1      37\n",
            "BsmtFinType2      38\n",
            "Electrical         1\n",
            "FireplaceQu      690\n",
            "GarageType        81\n",
            "GarageYrBlt       81\n",
            "GarageFinish      81\n",
            "GarageQual        81\n",
            "GarageCond        81\n",
            "PoolQC          1453\n",
            "Fence           1179\n",
            "MiscFeature     1406\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = randomize_dataset(train)\n",
        "X1,Y1 = ConvertToInputOutput(train)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = obtain_training_test_data(X1, Y1, 0.3)\n",
        "\n",
        "#Normalisation\n",
        "X_train = Normalize(X_train)\n",
        "X_test = Normalize(X_test)\n",
        "Y_train = Normalize(Y_train)\n",
        "Y_test = Normalize(Y_test)\n",
        "X_train = X_train.to_numpy()\n",
        "Y_train = Y_train.to_numpy()\n",
        "X_test = X_test.to_numpy()\n",
        "Y_test = Y_test.to_numpy()\n",
        "print(\"[PREPROCESSING] Completed\")"
      ],
      "metadata": {
        "id": "_UC9mffNe1d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a79a2a-4f65-4b8d-e413-3a1435eededf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PREPROCESSING] Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression():\n",
        "  def __init__(self):\n",
        "    self.loss = []\n",
        "\n",
        "  def fit(self,X,Y,learningRate = 0.1,numIterations=10):\n",
        "    m,n = X.shape\n",
        "    self.outputstd = Y.std()\n",
        "    self.outputmean = Y.mean()\n",
        "    self.weights = np.ones((n, 1))\n",
        "    self.bias = 0\n",
        "\n",
        "    for i  in range(numIterations):\n",
        "      predY = np.dot(X, self.weights)+self.bias\n",
        "      error = Y -predY\n",
        "      mse = np.square(error).mean()\n",
        "\n",
        "      self.loss.append(mse)\n",
        "\n",
        "      dw = -(1/m)*np.dot(X.T, error)\n",
        "      db = -(1/m)*np.nansum(error,keepdims = True)\n",
        "      self.weights = self.weights - learningRate*dw\n",
        "      self.bias = self.bias - learningRate*db\n",
        "      if(i%10==0):\n",
        "        print(\"Iteration \", i + 1 ,\"/\",numIterations, \"MSE: \", mse)\n",
        "\n",
        "    print(self.loss)\n",
        "\n",
        "  def predict(self, X):\n",
        "    return (np.dot(X, self.weights) + self.bias) * (self.outputstd + 0.001) + self.outputmean\n",
        "\n",
        "  def valid(self,X,Y):\n",
        "    predY = np.dot(X, self.weights) + self.bias\n",
        "    error = Y - predY\n",
        "    mse = np.square(error).mean()\n",
        "    print(mse)"
      ],
      "metadata": {
        "id": "v0IAeP6QoS92"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear = LinearRegression()\n",
        "linear.fit(X_train, Y_train, learningRate = 0.01, numIterations = 1000)"
      ],
      "metadata": {
        "id": "wOVR5yuHraWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b3333f-c672-40d7-ac09-5cea2e2f01db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  1 / 1000 MSE:  13.786066745768053\n",
            "Iteration  11 / 1000 MSE:  6.9710984078374505\n",
            "Iteration  21 / 1000 MSE:  3.5877659942949105\n",
            "Iteration  31 / 1000 MSE:  1.9069202567561536\n",
            "Iteration  41 / 1000 MSE:  1.07082487466749\n",
            "Iteration  51 / 1000 MSE:  0.6539898907025571\n",
            "Iteration  61 / 1000 MSE:  0.44533396511442125\n",
            "Iteration  71 / 1000 MSE:  0.3401310987929162\n",
            "Iteration  81 / 1000 MSE:  0.2864131516842576\n",
            "Iteration  91 / 1000 MSE:  0.2583838026208579\n",
            "Iteration  101 / 1000 MSE:  0.243230886223653\n",
            "Iteration  111 / 1000 MSE:  0.23458469374103674\n",
            "Iteration  121 / 1000 MSE:  0.2292728726583296\n",
            "Iteration  131 / 1000 MSE:  0.2257105539732573\n",
            "Iteration  141 / 1000 MSE:  0.2231016222661854\n",
            "Iteration  151 / 1000 MSE:  0.2210423584069431\n",
            "Iteration  161 / 1000 MSE:  0.21932469139438973\n",
            "Iteration  171 / 1000 MSE:  0.21783848059006308\n",
            "Iteration  181 / 1000 MSE:  0.2165229361556625\n",
            "Iteration  191 / 1000 MSE:  0.2153424276384688\n",
            "Iteration  201 / 1000 MSE:  0.21427440208518306\n",
            "Iteration  211 / 1000 MSE:  0.21330331948243042\n",
            "Iteration  221 / 1000 MSE:  0.21241758217823264\n",
            "Iteration  231 / 1000 MSE:  0.21160795737373916\n",
            "Iteration  241 / 1000 MSE:  0.21086674711706802\n",
            "Iteration  251 / 1000 MSE:  0.21018733505147066\n",
            "Iteration  261 / 1000 MSE:  0.20956392522012776\n",
            "Iteration  271 / 1000 MSE:  0.20899138062655379\n",
            "Iteration  281 / 1000 MSE:  0.20846511517585564\n",
            "Iteration  291 / 1000 MSE:  0.20798101548424122\n",
            "Iteration  301 / 1000 MSE:  0.20753538045429346\n",
            "Iteration  311 / 1000 MSE:  0.2071248722332121\n",
            "Iteration  321 / 1000 MSE:  0.20674647505981764\n",
            "Iteration  331 / 1000 MSE:  0.20639745998315157\n",
            "Iteration  341 / 1000 MSE:  0.20607535420617906\n",
            "Iteration  351 / 1000 MSE:  0.20577791422308733\n",
            "Iteration  361 / 1000 MSE:  0.20550310215268652\n",
            "Iteration  371 / 1000 MSE:  0.2052490648107782\n",
            "Iteration  381 / 1000 MSE:  0.20501411515491988\n",
            "Iteration  391 / 1000 MSE:  0.20479671579801978\n",
            "Iteration  401 / 1000 MSE:  0.20459546433407558\n",
            "Iteration  411 / 1000 MSE:  0.20440908025614032\n",
            "Iteration  421 / 1000 MSE:  0.20423639327654053\n",
            "Iteration  431 / 1000 MSE:  0.20407633288436816\n",
            "Iteration  441 / 1000 MSE:  0.20392791899646115\n",
            "Iteration  451 / 1000 MSE:  0.20379025357624395\n",
            "Iteration  461 / 1000 MSE:  0.20366251311044517\n",
            "Iteration  471 / 1000 MSE:  0.20354394184724744\n",
            "Iteration  481 / 1000 MSE:  0.2034338457111758\n",
            "Iteration  491 / 1000 MSE:  0.20333158682024713\n",
            "Iteration  501 / 1000 MSE:  0.20323657853980237\n",
            "Iteration  511 / 1000 MSE:  0.20314828101520446\n",
            "Iteration  521 / 1000 MSE:  0.20306619713236573\n",
            "Iteration  531 / 1000 MSE:  0.20298986886099313\n",
            "Iteration  541 / 1000 MSE:  0.20291887394063143\n",
            "Iteration  551 / 1000 MSE:  0.2028528228741298\n",
            "Iteration  561 / 1000 MSE:  0.20279135619714933\n",
            "Iteration  571 / 1000 MSE:  0.2027341419958336\n",
            "Iteration  581 / 1000 MSE:  0.20268087364784712\n",
            "Iteration  591 / 1000 MSE:  0.2026312677647023\n",
            "Iteration  601 / 1000 MSE:  0.20258506231568638\n",
            "Iteration  611 / 1000 MSE:  0.20254201491581314\n",
            "Iteration  621 / 1000 MSE:  0.2025019012620909\n",
            "Iteration  631 / 1000 MSE:  0.20246451370404658\n",
            "Iteration  641 / 1000 MSE:  0.2024296599359115\n",
            "Iteration  651 / 1000 MSE:  0.20239716179916756\n",
            "Iteration  661 / 1000 MSE:  0.20236685418530614\n",
            "Iteration  671 / 1000 MSE:  0.202338584029675\n",
            "Iteration  681 / 1000 MSE:  0.20231220938819727\n",
            "Iteration  691 / 1000 MSE:  0.20228759858956114\n",
            "Iteration  701 / 1000 MSE:  0.2022646294561997\n",
            "Iteration  711 / 1000 MSE:  0.2022431885880288\n",
            "Iteration  721 / 1000 MSE:  0.20222317070348808\n",
            "Iteration  731 / 1000 MSE:  0.2022044780329474\n",
            "Iteration  741 / 1000 MSE:  0.20218701976000608\n",
            "Iteration  751 / 1000 MSE:  0.20217071150662666\n",
            "Iteration  761 / 1000 MSE:  0.20215547485841978\n",
            "Iteration  771 / 1000 MSE:  0.20214123692673308\n",
            "Iteration  781 / 1000 MSE:  0.2021279299444986\n",
            "Iteration  791 / 1000 MSE:  0.20211549089306727\n",
            "Iteration  801 / 1000 MSE:  0.20210386115750328\n",
            "Iteration  811 / 1000 MSE:  0.20209298620803615\n",
            "Iteration  821 / 1000 MSE:  0.20208281530556563\n",
            "Iteration  831 / 1000 MSE:  0.20207330122930015\n",
            "Iteration  841 / 1000 MSE:  0.202064400024771\n",
            "Iteration  851 / 1000 MSE:  0.20205607077061663\n",
            "Iteration  861 / 1000 MSE:  0.2020482753626645\n",
            "Iteration  871 / 1000 MSE:  0.20204097831396298\n",
            "Iteration  881 / 1000 MSE:  0.20203414656952728\n",
            "Iteration  891 / 1000 MSE:  0.20202774933466502\n",
            "Iteration  901 / 1000 MSE:  0.20202175791583993\n",
            "Iteration  911 / 1000 MSE:  0.20201614557311753\n",
            "Iteration  921 / 1000 MSE:  0.20201088738331346\n",
            "Iteration  931 / 1000 MSE:  0.20200596011303515\n",
            "Iteration  941 / 1000 MSE:  0.20200134210087353\n",
            "Iteration  951 / 1000 MSE:  0.20199701314805865\n",
            "Iteration  961 / 1000 MSE:  0.2019929544169484\n",
            "Iteration  971 / 1000 MSE:  0.20198914833676856\n",
            "Iteration  981 / 1000 MSE:  0.20198557851606727\n",
            "Iteration  991 / 1000 MSE:  0.20198222966138943\n",
            "[13.786066745768053, 12.87065915782194, 12.017191242335807, 11.221469464604734, 10.479584234656393, 9.787890680974211, 9.142990726062168, 8.541716375701332, 7.981114139717152, 7.4584305076410065, 6.9710984078374505, 6.516724583505056, 6.093077823467862, 5.698077989878159, 5.329785788870452, 4.986393233860166, 4.666214754587005, 4.367678908178519, 4.089320651470139, 3.8297741365780977, 3.5877659942949105, 3.3621090722761937, 3.1516965972241344, 2.955496732358145, 2.7725475034071465, 2.601952068170299, 2.4428743063826124, 2.2945347081970637, 2.156206541063443, 2.027212276153217, 1.9069202567561536, 1.794741592264403, 1.6901272624691552, 1.592565417929275, 1.5015788631355704, 1.4167227100933129, 1.3375821907837246, 1.263770617746484, 1.194927482753745, 1.1307166842252814, 1.07082487466749, 1.0149599200092532, 0.9628494632579505, 0.9142395854119435, 0.8688935570441473, 0.8265906744172102, 0.7871251744065414, 0.7503052228949838, 0.7159519716642586, 0.6838986791451648, 0.6539898907025571, 0.6260806744239185, 0.6000359086532974, 0.5757296177668523, 0.5530443529234976, 0.5318706147453218, 0.5121063150886507, 0.4936562752588762, 0.47643175820139066, 0.4603500323680568, 0.44533396511442125, 0.4313116436281032, 0.41821602152418735, 0.40598458936967186, 0.3945590675167063, 0.38388511973406164, 0.37391208622855704, 0.36459273474352716, 0.35588302851030956, 0.347741909911616, 0.3401310987929162, 0.33301490443000037, 0.3263600502280474, 0.3201355102901312, 0.31431235705147714, 0.30886361923019146, 0.30376414939592866, 0.2989905005052534, 0.2945208107965585, 0.2903346964785014, 0.2864131516842576, 0.2827384551996142, 0.2792940835062436, 0.27606462971255136, 0.2730357279734473, 0.2701939830273819, 0.2675269045041546, 0.26502284568046447, 0.2626709463820429, 0.26046107975160443, 0.2583838026208579, 0.25643030924254834, 0.2545923881550214, 0.2528623819672056, 0.2512331498662743, 0.2496980326636328, 0.24825082020736214, 0.24688572100088912, 0.24559733387849808, 0.24438062159841903, 0.243230886223653, 0.24214374616949025, 0.24111511480486975, 0.2401411805023729, 0.23921838803876477, 0.2383434212546415, 0.23751318688793, 0.23672479950176203, 0.23597556743262477, 0.23526297968970672, 0.23458469374103674, 0.2339385241263738, 0.23332243184087034, 0.2327345144373231, 0.23217299679835876, 0.23163622253319507, 0.23112264595669058, 0.23063082461125908, 0.23015941229489326, 0.22970715256103272, 0.2292728726583296, 0.2288554778805295, 0.22845394629870122, 0.22806732384992942, 0.22769471975833658, 0.22733530226593512, 0.22698829465233344, 0.2266529715237405, 0.22632865535303753, 0.22601471325391895, 0.2257105539732573, 0.22541562508691815, 0.22512941038525197, 0.2248514274354231, 0.22458122530860358, 0.22431838246087257, 0.22406250475741538, 0.22381322363032277, 0.22357019436094555, 0.22333309447837466, 0.2231016222661854, 0.22287549537011758, 0.22265444949985974, 0.22243823721856854, 0.22222662681418337, 0.22201940124700234, 0.2218163571683562, 0.22161730400556937, 0.2214220631087218, 0.22123046695502943, 0.2210423584069431, 0.22085759002033217, 0.22067602339936246, 0.22049752859490918, 0.22032198354355975, 0.22014927354445898, 0.21997929077143755, 0.21981193381803546, 0.2196471072731959, 0.21948472132555463, 0.21932469139438973, 0.21916693778542953, 0.21901138536983666, 0.21885796328480073, 0.21870660465427838, 0.21855724632851897, 0.21840982864110378, 0.21826429518231713, 0.21812059258774252, 0.21797867034105725, 0.21783848059006308, 0.2176999779750606, 0.2175631194687316, 0.21742786422675237, 0.217294173448413, 0.21716201024656623, 0.21703133952627562, 0.2169021278715764, 0.2167743434397996, 0.21664795586295027, 0.2165229361556625, 0.2163992566292878, 0.21627689081170262, 0.21615581337244974, 0.21603600005285276, 0.21591742760076882, 0.21580007370966706, 0.21568391696174025, 0.21556893677477906, 0.2154551133525537, 0.2153424276384688, 0.21523086127226893, 0.2151203965495911, 0.2150110163841716, 0.21490270427252917, 0.21479544426095706, 0.21468922091467046, 0.21458401928896254, 0.21447982490223516, 0.21437662371077876, 0.21427440208518306, 0.2141731467882698, 0.21407284495444517, 0.2139734840703776, 0.21387505195691056, 0.21377753675212974, 0.21368092689550533, 0.21358521111303969, 0.21349037840335125, 0.21339641802463347, 0.21330331948243042, 0.21321107251817362, 0.21311966709843078, 0.21302909340481838, 0.21293934182453364, 0.2128504029414658, 0.21276226752784758, 0.2126749265364111, 0.21258837109301548, 0.21250259248971395, 0.21241758217823264, 0.21233333176383326, 0.21224983299953443, 0.21216707778066865, 0.21208505813975229, 0.21200376624164863, 0.21192319437900423, 0.21184333496794197, 0.21176418054399268, 0.21168572375825073, 0.21160795737373916, 0.2115308742619704, 0.2114544673996901, 0.2113787298657928, 0.21130365483839764, 0.21122923559207477, 0.21115546549521202, 0.21108233800751394, 0.21100984667762354, 0.21093798514086062, 0.21086674711706802, 0.2107961264085597, 0.21072611689816442, 0.2106567125473586, 0.21058790739448358, 0.2105196955530414, 0.2104520712100649, 0.21038502862455746, 0.2103185621259977, 0.2102526661129064, 0.21018733505147066, 0.21012256347422284, 0.21005834597877057, 0.20999467722657544, 0.20993155194177637, 0.20986896491005688, 0.20980691097755202, 0.2097453850497941, 0.2096843820906938, 0.20962389712155605, 0.20956392522012776, 0.20950446151967614, 0.20944550120809563, 0.2093870395270425, 0.20932907177109503, 0.2092715932869387, 0.20921459947257404, 0.2091580857765471, 0.20910204769720075, 0.2090464807819464, 0.20899138062655379, 0.20893674287446004, 0.20888256321609525, 0.2088288373882249, 0.2087755611733079, 0.20872273039886946, 0.20867034093688913, 0.2086183887032012, 0.20856686965690988, 0.20851577979981586, 0.20846511517585564, 0.2084148718705522, 0.2083650460104768, 0.20831563376272214, 0.2082666313343847, 0.20821803497205812, 0.20816984096133564, 0.20812204562632172, 0.20807464532915293, 0.20802763646952718, 0.20798101548424122, 0.20793477884673675, 0.20788892306665355, 0.20784344468939053, 0.20779834029567448, 0.20775360650113525, 0.20770923995588816, 0.20766523734412323, 0.2076215953837008, 0.20757831082575331, 0.20753538045429346, 0.2074928010858284, 0.20745056956897945, 0.2074086827841082, 0.20736713764294712, 0.2073259310882366, 0.2072850600933667, 0.2072445216620243, 0.2072043128278452, 0.20716443065407125, 0.2071248722332121, 0.20708563468671204, 0.20704671516462125, 0.20700811084527132, 0.2069698189349559, 0.20693183666761492, 0.20689416130452357, 0.20685679013398542, 0.20681972047102948, 0.20678294965711114, 0.20674647505981764, 0.20671029407257688, 0.20667440411437002, 0.20663880262944823, 0.20660348708705306, 0.20656845498113965, 0.20653370383010472, 0.20649923117651706, 0.20646503458685186, 0.20643111165122863, 0.20639745998315157, 0.2063640772192546, 0.20633096101904816, 0.2062981090646701, 0.2062655190606395, 0.20623318873361327, 0.20620111583214606, 0.20616929812645304, 0.20613773340817562, 0.20610641949015016, 0.20607535420617906, 0.20604453541080553, 0.20601396097909006, 0.20598362880639043, 0.20595353680814416, 0.20592368291965335, 0.2058940650958724, 0.20586468131119817, 0.20583552955926288, 0.20580660785272903, 0.20577791422308733, 0.2057494467204568, 0.20572120341338707, 0.20569318238866333, 0.20566538175111376, 0.2056377996234185, 0.20561043414592176, 0.20558328347644572, 0.20555634579010656, 0.205529619279133, 0.20550310215268652, 0.20547679263668425, 0.20545068897362337, 0.205424789422408, 0.20539909225817793, 0.20537359577213946, 0.20534829827139794, 0.20532319807879273, 0.2052982935327336, 0.20527358298703932, 0.2052490648107782, 0.20522473738811015, 0.20520059911813074, 0.20517664841471728, 0.20515288370637627, 0.20512930343609295, 0.2051059060611823, 0.2050826900531422, 0.20505965389750758, 0.20503679609370684, 0.20501411515491988, 0.2049916096079373, 0.2049692779930216, 0.20494711886377, 0.20492513078697847, 0.20490331234250764, 0.20488166212315015, 0.20486017873449938, 0.20483886079481978, 0.20481770693491874, 0.20479671579801978, 0.20477588603963703, 0.20475521632745164, 0.20473470534118898, 0.20471435177249758, 0.20469415432482932, 0.2046741117133211, 0.20465422266467753, 0.20463448591705527, 0.2046149002199485, 0.20459546433407558, 0.20457617703126724, 0.2045570370943558, 0.20453804331706552, 0.20451919450390463, 0.20450048947005803, 0.20448192704128154, 0.204463506053797, 0.20444522535418902, 0.20442708379930227, 0.20440908025614032, 0.20439121360176551, 0.20437348272320002, 0.20435588651732747, 0.20433842389079654, 0.20432109375992466, 0.20430389505060378, 0.204286826698206, 0.2042698876474913, 0.20425307685251567, 0.20423639327654053, 0.20421983589194267, 0.20420340368012582, 0.20418709563143292, 0.20417091074505894, 0.20415484802896527, 0.20413890649979483, 0.20412308518278763, 0.20410738311169815, 0.20409179932871266, 0.20407633288436816, 0.2040609828374718, 0.2040457482550212, 0.20403062821212584, 0.20401562179192903, 0.20400072808553088, 0.20398594619191215, 0.20397127521785854, 0.2039567142778863, 0.20394226249416836, 0.20392791899646115, 0.20391368292203246, 0.2038995534155898, 0.20388552962920992, 0.20387161072226842, 0.20385779586137076, 0.20384408422028377, 0.2038304749798676, 0.20381696732800875, 0.20380356045955392, 0.20379025357624395, 0.20377704588664902, 0.20376393660610437, 0.2037509249566466, 0.20373801016695064, 0.2037251914722676, 0.20371246811436308, 0.20369983934145583, 0.2036873044081579, 0.20367486257541462, 0.20366251311044517, 0.2036502552866847, 0.20363808838372596, 0.20362601168726202, 0.20361402448902985, 0.2036021260867537, 0.20359031578409012, 0.2035785928905726, 0.2035669567215571, 0.20355540659816862, 0.20354394184724744, 0.20353256180129664, 0.2035212657984298, 0.20351005318231932, 0.20349892330214545, 0.20348787551254524, 0.20347690917356287, 0.20346602365059996, 0.20345521831436614, 0.20344449254083108, 0.2034338457111758, 0.20342327721174527, 0.2034127864340015, 0.20340237277447654, 0.20339203563472652, 0.20338177442128588, 0.20337158854562215, 0.20336147742409108, 0.20335144047789228, 0.20334147713302553, 0.20333158682024713, 0.20332176897502696, 0.20331202303750565, 0.2033023484524529, 0.2032927446692253, 0.20328321114172532, 0.2032737473283601, 0.20326435269200113, 0.20325502669994416, 0.2032457688238694, 0.20323657853980237, 0.20322745532807476, 0.20321839867328603, 0.20320940806426532, 0.2032004829940335, 0.20319162295976595, 0.20318282746275548, 0.20317409600837552, 0.20316542810604393, 0.20315682326918713, 0.20314828101520446, 0.2031398008654328, 0.2031313823451117, 0.2031230249833492, 0.20311472831308683, 0.2031064918710666, 0.20309831519779678, 0.20309019783751914, 0.2030821393381756, 0.203074139251376, 0.20306619713236573, 0.20305831253999362, 0.20305048503668055, 0.20304271418838798, 0.20303499956458684, 0.20302734073822687, 0.20301973728570627, 0.20301218878684138, 0.20300469482483693, 0.20299725498625648, 0.20298986886099313, 0.2029825360422403, 0.20297525612646353, 0.20296802871337133, 0.20296085340588765, 0.20295372981012352, 0.20294665753534968, 0.2029396361939691, 0.20293266540148966, 0.2029257447764978, 0.20291887394063143, 0.20291205251855374, 0.20290528013792722, 0.2028985564293876, 0.20289188102651837, 0.20288525356582537, 0.20287867368671184, 0.20287214103145315, 0.20286565524517247, 0.20285921597581616, 0.2028528228741298, 0.20284647559363397, 0.20284017379060063, 0.2028339171240296, 0.20282770525562538, 0.20282153784977366, 0.20281541457351904, 0.20280933509654187, 0.20280329909113617, 0.20279730623218717, 0.20279135619714933, 0.20278544866602477, 0.20277958332134113, 0.20277375984813073, 0.20276797793390902, 0.2027622372686536, 0.20275653754478354, 0.2027508784571384, 0.20274525970295834, 0.2027396809818634, 0.2027341419958336, 0.202728642449189, 0.20272318204857023, 0.20271776050291862, 0.2027123775234571, 0.20270703282367103, 0.2027017261192891, 0.2026964571282646, 0.20269122557075672, 0.20268603116911227, 0.20268087364784712, 0.20267575273362815, 0.2026706681552554, 0.20266561964364418, 0.20266060693180743, 0.20265562975483814, 0.2026506878498923, 0.2026457809561713, 0.2026409088149055, 0.20263607116933682, 0.2026312677647023, 0.20262649834821767, 0.20262176266906048, 0.2026170604783545, 0.20261239152915309, 0.2026077555764234, 0.20260315237703058, 0.2025985816897221, 0.20259404327511213, 0.20258953689566636, 0.20258506231568638, 0.2025806193012949, 0.20257620762042058, 0.20257182704278315, 0.20256747733987882, 0.20256315828496566, 0.20255886965304898, 0.20255461122086724, 0.20255038276687753, 0.20254618407124195, 0.20254201491581314, 0.2025378750841209, 0.2025337643613581, 0.2025296825343673, 0.20252562939162738, 0.20252160472324002, 0.20251760832091645, 0.2025136399779644, 0.2025096994892752, 0.20250578665131078, 0.2025019012620909, 0.20249804312118044, 0.20249421202967716, 0.20249040779019875, 0.2024866302068711, 0.20248287908531543, 0.2024791542326368, 0.20247545545741166, 0.20247178256967624, 0.20246813538091446, 0.20246451370404658, 0.20246091735341726, 0.20245734614478436, 0.20245379989530732, 0.20245027842353605, 0.20244678154939966, 0.20244330909419525, 0.20243986088057717, 0.20243643673254585, 0.20243303647543714, 0.2024296599359115, 0.20242630694194352, 0.20242297732281117, 0.2024196709090856, 0.2024163875326206, 0.20241312702654238, 0.20240988922523945, 0.2024066739643525, 0.20240348108076436, 0.20240031041259018, 0.20239716179916756, 0.20239403508104659, 0.20239093009998063, 0.2023878466989163, 0.20238478472198432, 0.20238174401448975, 0.20237872442290292, 0.20237572579485028, 0.20237274797910468, 0.20236979082557688, 0.20236685418530614, 0.20236393791045157, 0.20236104185428286, 0.2023581658711719, 0.20235530981658378, 0.20235247354706834, 0.20234965692025142, 0.20234685979482647, 0.20234408203054605, 0.2023413234882136, 0.202338584029675, 0.20233586351781047, 0.20233316181652625, 0.20233047879074684, 0.20232781430640662, 0.20232516823044225, 0.20232254043078438, 0.20231993077635035, 0.20231733913703598, 0.20231476538370818, 0.20231220938819727, 0.20230967102328928, 0.20230715016271875, 0.20230464668116088, 0.20230216045422472, 0.2022996913584453, 0.202297239271277, 0.20229480407108555, 0.20229238563714178, 0.202289983849614, 0.20228759858956114, 0.20228522973892588, 0.2022828771805277, 0.20228054079805605, 0.20227822047606347, 0.2022759160999592, 0.20227362755600206, 0.20227135473129437, 0.20226909751377478, 0.2022668557922123, 0.2022646294561997, 0.20226241839614695, 0.20226022250327508, 0.2022580416696098, 0.2022558757879753, 0.20225372475198805, 0.20225158845605068, 0.20224946679534597, 0.20224735966583046, 0.20224526696422912, 0.2022431885880288, 0.20224112443547276, 0.20223907440555444, 0.20223703839801208, 0.20223501631332275, 0.20223300805269667, 0.20223101351807157, 0.20222903261210723, 0.20222706523817965, 0.20222511130037582, 0.20222317070348808, 0.20222124335300873, 0.20221932915512467, 0.2022174280167121, 0.2022155398453312, 0.20221366454922085, 0.20221180203729344, 0.20220995221912982, 0.20220811500497393, 0.202206290305728, 0.2022044780329474, 0.20220267809883558, 0.202200890416239, 0.20219911489864278, 0.2021973514601651, 0.2021956000155527, 0.20219386048017624, 0.20219213277002518, 0.2021904168017034, 0.20218871249242426, 0.20218701976000608, 0.20218533852286757, 0.2021836687000231, 0.20218201021107837, 0.20218036297622569, 0.20217872691623973, 0.20217710195247293, 0.20217548800685126, 0.20217388500186975, 0.20217229286058808, 0.20217071150662666, 0.20216914086416188, 0.20216758085792233, 0.20216603141318437, 0.202164492455768, 0.20216296391203292, 0.2021614457088741, 0.20215993777371816, 0.20215844003451897, 0.2021569524197539, 0.20215547485841978, 0.2021540072800289, 0.20215254961460538, 0.2021511017926809, 0.20214966374529125, 0.2021482354039723, 0.20214681670075643, 0.20214540756816846, 0.20214400793922224, 0.20214261774741685, 0.20214123692673308, 0.20213986541162943, 0.20213850313703902, 0.2021371500383657, 0.2021358060514805, 0.2021344711127183, 0.20213314515887423, 0.2021318281272003, 0.2021305199554017, 0.20212922058163385, 0.2021279299444986, 0.20212664798304122, 0.20212537463674685, 0.20212410984553716, 0.20212285354976733, 0.20212160569022275, 0.20212036620811544, 0.2021191350450814, 0.20211791214317706, 0.20211669744487634, 0.20211549089306727, 0.20211429243104928, 0.20211310200252977, 0.20211191955162136, 0.2021107450228385, 0.2021095783610949, 0.20210841951170028, 0.20210726842035742, 0.2021061250331593, 0.2021049892965864, 0.20210386115750328, 0.20210274056315636, 0.20210162746117036, 0.20210052179954632, 0.20209942352665808, 0.20209833259124987, 0.2020972489424336, 0.2020961725296859, 0.2020951033028455, 0.20209404121211053, 0.20209298620803615, 0.20209193824153138, 0.20209089726385673, 0.20208986322662179, 0.2020888360817823, 0.2020878157816378, 0.202086802278829, 0.2020857955263352, 0.2020847954774721, 0.20208380208588878, 0.20208281530556563, 0.20208183509081196, 0.20208086139626313, 0.20207989417687855, 0.20207893338793922, 0.2020779789850451, 0.20207703092411308, 0.20207608916137446, 0.2020751536533727, 0.20207422435696096, 0.20207330122930015, 0.20207238422785626, 0.2020714733103984, 0.20207056843499652, 0.20206966956001898, 0.20206877664413075, 0.20206788964629077, 0.20206700852575016, 0.20206613324204997, 0.20206526375501876, 0.202064400024771, 0.20206354201170457, 0.2020626896764988, 0.20206184298011243, 0.20206100188378154, 0.20206016634901752, 0.202059336337605, 0.20205851181160003, 0.20205769273332772, 0.20205687906538064, 0.20205607077061663, 0.20205526781215713, 0.20205447015338482, 0.20205367775794206, 0.20205289058972883, 0.202052108612901, 0.20205133179186818, 0.2020505600912922, 0.2020497934760849, 0.20204903191140672, 0.2020482753626645, 0.20204752379550991, 0.20204677717583758, 0.20204603546978328, 0.2020452986437224, 0.2020445666642677, 0.20204383949826826, 0.20204311711280712, 0.20204239947520003, 0.2020416865529934, 0.20204097831396298, 0.20204027472611183, 0.20203957575766893, 0.20203888137708742, 0.20203819155304292, 0.2020375062544321, 0.20203682545037085, 0.20203614911019285, 0.20203547720344783, 0.20203480969990018, 0.20203414656952728, 0.20203348778251792, 0.20203283330927088, 0.20203218312039323, 0.2020315371866991, 0.2020308954792079, 0.20203025796914287, 0.20202962462792978, 0.20202899542719535, 0.20202837033876567, 0.20202774933466502, 0.20202713238711423, 0.20202651946852923, 0.2020259105515198, 0.20202530560888815, 0.20202470461362734, 0.2020241075389201, 0.20202351435813734, 0.20202292504483682, 0.20202233957276186, 0.20202175791583993, 0.20202118004818131, 0.20202060594407775, 0.20202003557800127, 0.2020194689246028, 0.2020189059587108, 0.20201834665533017, 0.20201779098964068, 0.20201723893699605, 0.20201669047292253, 0.20201614557311753, 0.2020156042134487, 0.2020150663699523, 0.2020145320188324, 0.20201400113645934, 0.20201347369936876, 0.2020129496842602, 0.20201242906799616, 0.20201191182760073, 0.20201139794025852, 0.20201088738331346, 0.20201038013426773, 0.20200987617078045, 0.2020093754706668, 0.2020088780118967, 0.20200838377259392, 0.20200789273103445, 0.20200740486564617, 0.20200692015500718, 0.20200643857784492, 0.20200596011303515, 0.2020054847396007, 0.2020050124367106, 0.202004543183679, 0.20200407695996414, 0.2020036137451671, 0.2020031535190311, 0.20200269626144027, 0.20200224195241867, 0.2020017905721294, 0.20200134210087353, 0.20200089651908895, 0.20200045380734963, 0.2020000139463647, 0.20199957691697715, 0.20199914270016311, 0.20199871127703092, 0.20199828262882003, 0.20199785673690024, 0.2019974335827706, 0.20199701314805865, 0.20199659541451923, 0.20199618036403405, 0.20199576797861019, 0.20199535824037976, 0.20199495113159854, 0.20199454663464553, 0.20199414473202165, 0.20199374540634915, 0.20199334864037066, 0.2019929544169484, 0.2019925627190632, 0.2019921735298137, 0.20199178683241556, 0.20199140261020065, 0.20199102084661605, 0.20199064152522347, 0.20199026462969827, 0.2019898901438286, 0.20198951805151485, 0.20198914833676856, 0.2019887809837119, 0.20198841597657655, 0.2019880532997033, 0.20198769293754093, 0.20198733487464582, 0.20198697909568067, 0.2019866255854143, 0.20198627432872054, 0.20198592531057766, 0.20198557851606727, 0.20198523393037426, 0.20198489153878538, 0.20198455132668897, 0.20198421327957403, 0.20198387738302953, 0.20198354362274376, 0.20198321198450359, 0.20198288245419377, 0.20198255501779624, 0.20198222966138943, 0.2019819063711476, 0.20198158513334022, 0.20198126593433105, 0.2019809487605778, 0.2019806335986313, 0.20198032043513478, 0.20198000925682333, 0.2019797000505232, 0.20197939280315125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear.valid(X_test, Y_test)"
      ],
      "metadata": {
        "id": "qaYXWbh4tLnj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a6461c-9ab2-46c2-d9c5-21bda4941915"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.31096374791510545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: Logistic Regression\n",
        "Welcome to your second question! Let us introduce you to the question.\n",
        "\n",
        "\n",
        "---\n",
        "You are provided a dataset which is used to predict if a patient has Breast Cancer or not. Your mission, should you choose to accept it, is to use the dataset to create a Logistic Regression model to predict whether a patient is benign / malignant. Try to obtain the highest accuracy.\n",
        "\n",
        "Dataset: https://drive.google.com/drive/folders/1jTnYiFaUn0czGEmOS637SWgwaNdSDp07?usp=sharing\n",
        "\n",
        "Resources to study:\n",
        "* https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
        "* https://www.scaler.com/topics/matplotlib/matplotlib-heatmap/\n",
        "* https://www.geeksforgeeks.org/understanding-logistic-regression/\n",
        "* https://www.analyticsvidhya.com/blog/2021/10/building-an-end-to-end-logistic-regression-model/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1kI29GqlZxNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sigmoid function\n",
        "def sigmoid(z):\n",
        "  return _______________________\n",
        "\n",
        "# Defining X and Y\n",
        "def GetInputOutputLR(dataset):\n",
        "  X = dataset.iloc[:, 0:-1]\n",
        "  Y = dataset.iloc[:, -1:]\n",
        "\n",
        "  return X, Y\n",
        "\n",
        "def obtain_training_test_data(X, Y, n):\n",
        "  # Your function! Learn to use pandas to split training and test data!"
      ],
      "metadata": {
        "id": "1z16LMAVrXYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in the blanks! You should be comfortable with numpy & pandas now.\n",
        "dataset = pd.read_csv(_________________________)\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "zMf-6-IN7V3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = GetInputOutputLR(dataset)\n",
        "\n",
        "# This dataset has 2, 4 as Benign & Malignant, so we convert it to 0 and 1.\n",
        "Y = Y.replace({_____________________})\n",
        "X_train, X_test, Y_train, Y_test = obtain_training_test_data(X, Y, 0.3);\n",
        "\n",
        "X_train.shape, Y_train.shape"
      ],
      "metadata": {
        "id": "Z8dwB53c0J51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmaps - What library uses heatmaps in Python?\n",
        "import _____________ as sns\n",
        "heatmap = sns.heatmap(dataset.corr(), vmin=-1, vmax=1, annot=True)\n",
        "heatmap.set_title('Correlation Heatmap');\n",
        "\n",
        "# Learn what a correlation heatmap is, we might ask you about this later on."
      ],
      "metadata": {
        "id": "drnkZc7o1DdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalisation - from Question 1, this cell is entirely yours. Add as many features / normalize and preprocess them to try to obtain maximum accuracy.\n",
        "#Pre-processing"
      ],
      "metadata": {
        "id": "Stohumoh1tLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Refer to Andrew NG's lectures of Logistic Regression to understand what's actually happening in terms of matrixs and weights. You can include bias here if you'd like, but we haven't included it\n",
        "# in this case. More points if you figure out how a bias' dimensions would work and actually implement it here.\n",
        "class LogisticRegression():\n",
        "  def __init__(self, learning_rate = 0.1, max_iterations=100):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.max_iterations = max_iterations\n",
        "    self.loss = []\n",
        "    self.w = []\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    self.w = np.zeros((_____________________), dtype=np.float32)\n",
        "\n",
        "    for iteration in range(_______________________):\n",
        "      dw, cost = self.gradient_cost_eval(self.w, X, Y)\n",
        "      self.w -= (self.learning_rate * dw)\n",
        "      self.loss.append(cost)\n",
        "\n",
        "      if iteration % 10 == 0:\n",
        "        print(\"Iteration \", iteration, \"/\", self.max_iterations, \", Loss: \", cost)\n",
        "\n",
        "# Do learn what (z), H(z) and such generic terms stand for, they are often used in the ML community.\n",
        "  def predict(self, X):\n",
        "    w = self.w\n",
        "    H = _________________\n",
        "\n",
        "# What threshold value should you have to say that the prediction is 1?\n",
        "    Y_pred = np.zeros(______________________)\n",
        "    for i in range(H.shape[1]):\n",
        "      if H[0, i] >= _________:\n",
        "        Y_pred[0,i] = 1\n",
        "      else:\n",
        "        Y_pred[0,i] = 0\n",
        "\n",
        "    return Y_pred\n",
        "\n",
        "  def test(self, X, Y):\n",
        "    Y_pred = self.predict(X)\n",
        "    print(\"Accuracy: \", ____________________________)\n",
        "\n",
        "  def hypo(self, w, X):\n",
        "    return sigmoid(__________________)\n",
        "\n",
        "# Use the binary crossentropy loss function here.\n",
        "  def cost(self, H, Y, num_samples):\n",
        "    return _______________________________________\n",
        "\n",
        "  def gradient_cost_eval(self, w, X, Y):\n",
        "    H = self.hypo(w, X)\n",
        "    cost = self.cost(H, Y, len(Y))\n",
        "\n",
        "    temp = (H - Y)\n",
        "    dw = np.dot(___________) / ______________\n",
        "\n",
        "    return dw, cost\n"
      ],
      "metadata": {
        "id": "LNKQ4-QF2XpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you define the Logistic Regression model now?\n",
        "LR = _________________________________\n",
        "LR.fit(________________)"
      ],
      "metadata": {
        "id": "nVr5b7Iz-EuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR.test(______________)"
      ],
      "metadata": {
        "id": "RACSOL8F_VP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submissions\n",
        "\n",
        "---\n",
        "\n",
        "You can submit your final solutions using this link:\n",
        "https://forms.gle/42UcG7dFttEStHyY8\n",
        "\n",
        "Thank you!"
      ],
      "metadata": {
        "id": "yhhIJx01cTGS"
      }
    }
  ]
}